# Redes Neurais Recorrentes

## V√≠deo 1 

https://www.youtube.com/watch?v=ZvBJxh5O3H0&pp=ygUZcmVkZXMgbmV1cmFpcyByZWNvcnJlbnRlcw%3D%3D

- Utiliza√ß√£o em dados sequ√™nciais - palavras, imagens, sons
    - Temperatura ao longo do tempo
- Prever a p≈ïoxima a√ß√£o
    - Em v√≠deos, previs√£o de frame atual com base nos frames anteriores
- Processamento de linguagem natural
    - Previs√£o da pr√≥xima palavra em um texto
    - Tradu√ß√£o autom√°tica (text to speech)
    - Gera√ß√£o de poemas
- Gera√ß√£o de legendas em v√≠deos
- S√©ries temporais (time-series)
    - Pre√ßo de a√ß√µes na bolsa de valores
    - Temperatura
    - Crescimento populacional
    - N√≠vel de polui√ß√£o

"Para entender o final de uma frase, voc√™ precisa saber o que foi dito antes."

- Redes Neurais tradicionais n√£o armazenam informa√ß√£o no tempo
- Redes Neurais Recorrentes utilizam loops que permitem que a informa√ß√£o persista -- passa a informa√ß√£o √† frente e manda para ele mesmo
- Desenrolar a rede neural -- recurs√£o 
- t n√∫mero de neur√¥nios
- M√∫ltiplas c√≥pias de si mesmo -- implica em processamento mais demorado

## V√≠deo 2

https://www.youtube.com/watch?v=elyOXSwL8xI

Sequ√™ncia: cole√ß√£o de elementos que podem ser repetidos e cuja ordem importa.

"Por que devemos nos importar com sequ√™ncias?"

- Tomam decis√µes baseadas n√£o s√≥ na entrada atual, mas em entradas anteriores
- Cada passo do desdobramento compartilha hiperpar√¢metros
- Cada passo considera a sa√≠da do passo anterior. Como numa recorr√™ncia, todo passo acaba sendo fun√ß√£o de todos os passos anteriores
- Treinamento √© feito atrav√©s de backpropagation atrav√©s do tempo
    - [Imagem]
    - Atrav√©s do tempo implica na adi√ß√£o do somat√≥rio na f√≥rmula, j√° que precisa calcular considerando todos os passos 
- Exemplo de utilidade: an√°lise de sentimento de uma frase/avalia√ß√£o. A representa√ß√£o gerada pelo √∫ltimo passo serve como entrada para um classificador, que determinar√° qual a sem√¢ntica da frase
    - Problema: ao utilizar apenas a √∫ltima informa√ß√£o, talvez possam ser perdidas informa√ß√µes anteriores.
    - Vanishing gradient
    - Solu√ß√£o: utilizar mais sa√≠das al√©m da √∫ltima, de alguma maneira. Por exemplo, com soma ponderada
- Exemplo de utilidade: codifica√ß√£o de pergunta para ser respondida. Dada essa codifica√ß√£o e algum contexto, os dois podem ser usados para gerar uma resposta
- Exemplo de utilidade: reconhecimento de fala. 
    - Sequence-to-sequence

Vanishing gradient: quando se tem muitos passos √© dif√≠cil propagar o erro ao treinar a rede
- Gradientes mais distantes s√£o muito menores que os de perto
- Com essa diferen√ßa pode ser dif√≠cil de capturar depend√™ncias entre informa√ß√µes distantes na sequ√™ncia
- [Imagem]
- LSTMs (Long Short-Term Memories) e GRUs (Gated Recurrent Networks)

## Livro

- Podem trabalhar com inputs de variados, como frases, documentos ou √°udio
- N√£o s√£o as √∫nicas que podem lidar com dados sequenciais
    - Para sequ√™ncias menores, redes densas tradicionais podem ser o suficiente
    - Para sequ√™ncias muito longas, convolucionais podem ser adequadas

### Neur√¥nios recorrentes e camadas

- Se parecem muito com redes feedforward, exceto que tamb√©m t√™m conex√µes apontando para tr√°s
- Cada time step pode ser chamado de frame (cada execu√ß√£o do loop)
- A cada frame, o neur√¥nio recorrente recebe uma entrada x(t) bem como a sa√≠da do √∫ltimo passo
    - Como n√£o h√° sa√≠da anterior para a primeira execu√ß√£o, geralmente ela √© setada como 0
    - [Imagem]
- Ambos entrada e sa√≠da s√£o vetores, ao passo que a sa√≠da era um escalar antes
- Cada neur√¥nio recorrente tem dois vetores de pesos, um para as entradas e outro para as sa√≠das dos passos anteriores
- ReLU ou hyperbolic tangent (tanh) como fun√ß√µes de ativa√ß√£o
- [Imagem f√≥rmula]

### C√©lulas de mem√≥ria

- Como a sa√≠da de um neur√¥nio recorrente em um dado tempo t √© fun√ß√£o das entradas dos frames anteriores, pode-se dizer que ela tem um tipo de mem√≥ria.
- A parte de uma rede neural que preserva algum estado atrav√©s dos frames √© chamada de c√©lula de mem√≥ria (ou apenas c√©lula)
- Um √∫nico neur√¥nio recorrente, ou uma camada deles, √© uma c√©lula muito b√°sica
- O estado de um neur√¥nio √© fun√ß√£o de alguns inputs naquele frame e do estado no frame anterior h(t) = f(x(t), h(t‚Äì1)).

### Input and Output Sequences

- Uma RNN pode tomar uma sequ√™ncia de inputs e retornar uma sequ√™ncia de outputs
    - √ötil para prever s√©ries temporais
        - Dado consumo energ√©tico de x dias, prever o consumo 
- Dada uma sequ√™ncia de inputs, pode-se tamb√©m ignorar todos os outputs exceto o √∫ltimo
    - Alimentada uma RNN com uma sequ√™ncia de palavras de review, receber como sa√≠da um score
- Alternativamente, pode-se alimentar a rede sempre com o mesmo input e ter como resultado uma sequ√™ncia
    - Input: imagem. Output: legenda
- Por √∫ltimo, existe a abordagem encoder-decoder, em que √© feita primeira a transi√ß√£o sequence-vector pelo encoder e depois a vector-sequence pelo decoder
    - Abordagem √∫til para tradu√ß√µes, especialmente porque as √∫ltimas palavras de uma sequ√™ncia podem afetar as primeiras de uma tradu√ß√£o -- o que n√£o seria considerado numa abordagem sequence-sequence

### Training RNNs

O treinamento √© realizado atrav√©s de backpropagation through time sobre a rede neural j√° desenrolada. 
1. Aplica√ß√£o de fun√ß√£o de c√°lculo de perda (loss function) considerando r√≥tulos e previs√µes (erro quadr√°tico, por exemplo)
\* Em algumas RNNs, algumas sa√≠das podem ser ignoradas. Por exemplo, em uma RNN sequence-to-vector apenas a √∫ltima sa√≠da √© considerada
2. Os gradientes da perda s√£o propagados de volta atrav√©s da rede desenrolada
3. Uma vez que a backword-phase foi completada e todos os gradientes foram computados, os par√¢metros podem ser atualizados usando os gradientes acumulados
\* [Imagem] onde ùúÇ √© a taxa de aprendizado, um hiperpar√¢metro que controla o tamanho do passo de atualiza√ß√£o

Keras √© uma biblioteca que toma conta dessa parte.

### Forecasting a Time Series

- Time-series: dados com valores em diferentes tempos, geralmente em intervalos regulares
- Multivariate time series: time series com m√∫ltiplos valores em cada tempo
- Univariate time series: time series com um √∫nico valor em cada tempo
- A atividade mais comum com time-series √© a *predi√ß√£o* do pr√≥ximo valor, mas outras atividades tamb√©m podem ser feitas:
    - Imputa√ß√£o
    - Classifica√ß√£o
    - Detec√ß√£o de anomalias
- Naive forecasting: quando a predi√ß√£o do pr√≥ximo valor √© uma c√≥pia de um valor do passado
    - Produz resultado satisfat√≥rio quando a sazonalidade √© muito forte

### Forecasting Using a Deep RNN

Deep RNN: m√∫ltiplas camadas de c√©lulas recorrentes

## Semin√°rio

- Bias geralmente come√ßa com 0
- Os pesos geralmente come√ßam com valores pequenos aleat√≥rios e s√£o ajustados durante o treinamento

#### Rede Neural Densa

Totalmente conectada ou feedforward, cada neur√¥nio em uma camada est√° conectado a cada neur√¥nio da camada seguinte.

#### Neur√¥nio

1. Recebe uma s√©rie de entradas
2. Cada entrada √© associada a um peso que determina sua import√¢ncia (pesos s√£o ajustados durante treinamento)
3. √â calculada a soma ponderada das entradas e pesos, adicionando um termo de vi√©s (bias) para ajustar a sa√≠da
4. A soma ponderada √© passada a uma fun√ß√£o de ativa√ß√£o que introduz n√£o-linearidade na rede e decide se o neur√¥nio deve "disparar" (produzir uma sa√≠da significativa)

## D√∫vidas

Fun√ß√£o de ativa√ß√£o?
Rede neural densa?
Ent√£o os pesos (de entrada, camadas ocultas e sa√≠da) s√£o por n√∫mero de unidades recorrentes? N√£o s√£o pelo n√∫mero de elementos na entrada porque isso faria que dependesse do tamanho da sequ√™ncia, e isso foi negado diversas vezes

## Problemas

- Tempo de processamento √© maior devido ao unfolding
- BPTT causa vanishing gradient ou explosion gradient